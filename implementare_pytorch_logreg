import torch 
from torch.autograd import Variable
import numpy as np
from matplotlib import pyplot as plt


class LogisticRegression(torch.nn.Module): #definim clasa pentru model
    def __init__(self, inputSize, outputSize):
        super(LogisticRegression, self).__init__()
        self.linear = torch.nn.Linear(inputSize, outputSize) #doar un layer cu bias

    def forward(self, x): # h(x) = t0 * x + t1
        out = torch.sigmoid(self.linear(x))
        return out


x_train, y_train = generate_data(30)

x_train =np.array([0.5, 1, 2, 5, 6, 7]) #nr ore de studiu pt examen
y_train = np.array([0,0,0,1,1,1]) #0-pica, 1-trecut

x_train = x_train[:, np.newaxis].astype(np.float32)
y_train = y_train[:, np.newaxis].astype(np.float32)

inputDim = 1
outputDim = 1

lr = 0.003
epochs = 500

model = LogisticRegression(inputDim, outputDim)

if torch.cuda.is_available(): #pe GPU
    model.cuda()

criterion = torch.nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=lr) #GD

losses = []
print(x_train.shape, y_train.shape)

for epoch in range(epochs):
    if torch.cuda.is_available():
        inputs = torch.from_numpy(x_train).cuda()
        labels = torch.from_numpy(y_train).cuda()
    else:
        inputs = torch.from_numpy(x_train)
        labels = torch.from_numpy(y_train)

    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward() # calculeaza gradientii
    optimizer.step()#update
    optimizer.zero_grad()
    losses.append(loss.item())

plt.title("loss")
plt.xlabel("epoci")
plt.ylabel("loss")
plt.plot(losses)
plt.show()








